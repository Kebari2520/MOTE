##same as part 2 script
dat = read.csv("Melted Data.csv")
colnames(dat)[2] = "Judgment.task"
colnames(dat)[4] = "Judgment"
##data screening
##same as part 2 script
dat = read.csv("Melted Data.csv")
##data screening
##same as part 2 script
dat = read.csv("Melted Data Thesis.csv")
names(data)
names(dat)
colnames(dat)[2] = "Judgment.task"
colnames(dat)[4] = "Judgment"
dat$Judgment[ dat$Judgment > 100 ] = NA
mahal = mahalanobis(dat[ , c(4,5)],
colMeans(dat[ , c(4,5)], na.rm = TRUE),
cov(dat[ , c(4,5)], use = "pairwise.complete.obs"))
cutoff = qchisq(1-.001, ncol(dat[ , c(4,5)]))
cutoff;ncol(dat[ , c(4,5)])
summary(mahal < cutoff)
dat.noout = subset(dat, mahal < cutoff)
summary(dat.noout)
##drop sw columns
dat.noout2 = dat.noout[ , c(1:8)]
##melt the data
long.dat = melt(dat.noout2, id = c("Partno", "Judgment.task", "Word.Pair", "COS", "LSA", "FSG"))
names(long.dat)
##rename columns
colnames(long.dat)[2] = "Judgment.Type"
##rename columns
colnames(long.dat)[2] = "Judgment.Type"
colnames(long.dat)[7] = "Task"
colnames(long.dat)[8] = "Score"
##make the graph
bar.plot = ggplot(long.dat, aes(Task, Score, fill = Judgment.Type)) +
cleanup +
theme(legend.position="bottom") +
stat_summary(fun.y = mean,
geom = "bar",
position = "dodge",
color = "black") +
stat_summary(fun.data = mean_cl_normal,
geom = "errorbar",
position = position_dodge(width = 0.90),
width = .2,
color = "darkgrey") +
scale_fill_manual("Judgment Type",
values = c("Associative" = "black",
"Semantic" = "dimgrey",
"Thematic" = "indianred4"))
cleanup = theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line.x = element_line(color = "black"),
axis.line.y = element_line(color = "black"),
legend.key = element_rect(fill = "white"),
text = element_text(size = 15))
##make the graph
bar.plot = ggplot(long.dat, aes(Task, Score, fill = Judgment.Type)) +
cleanup +
theme(legend.position="bottom") +
stat_summary(fun.y = mean,
geom = "bar",
position = "dodge",
color = "black") +
stat_summary(fun.data = mean_cl_normal,
geom = "errorbar",
position = position_dodge(width = 0.90),
width = .2,
color = "darkgrey") +
scale_fill_manual("Judgment Type",
values = c("Associative" = "black",
"Semantic" = "dimgrey",
"Thematic" = "indianred4"))
bar.plot
##updated graph
bar.plot2 = ggplot(long.dat, aes(Judgment.Type, Score, fill = Task)) +
cleanup +
theme(legend.position="bottom") +
stat_summary(fun.y = mean,
geom = "bar",
position = "dodge",
color = "black") +
stat_summary(fun.data = mean_cl_normal,
geom = "errorbar",
position = position_dodge(width = 0.90),
width = .2,
color = "black") +
scale_fill_manual("Task",
values = c("Judgment" = "indianred4",
"Recall" = "dimgray")) +
ylim(0, 1)
bar.plot2
dat.noout2$Judgment = dat.noout2$Judgment/100
##melt the data
long.dat = melt(dat.noout2, id = c("Partno", "Judgment.task", "Word.Pair", "COS", "LSA", "FSG"))
##rename columns
colnames(long.dat)[2] = "Judgment.Type"
colnames(long.dat)[7] = "Task"
colnames(long.dat)[8] = "Score"
##make the graph
bar.plot = ggplot(long.dat, aes(Task, Score, fill = Judgment.Type)) +
cleanup +
theme(legend.position="bottom") +
stat_summary(fun.y = mean,
geom = "bar",
position = "dodge",
color = "black") +
stat_summary(fun.data = mean_cl_normal,
geom = "errorbar",
position = position_dodge(width = 0.90),
width = .2,
color = "darkgrey") +
scale_fill_manual("Judgment Type",
values = c("Associative" = "black",
"Semantic" = "dimgrey",
"Thematic" = "indianred4"))
bar.plot
##updated graph
bar.plot2 = ggplot(long.dat, aes(Judgment.Type, Score, fill = Task)) +
cleanup +
theme(legend.position="bottom") +
stat_summary(fun.y = mean,
geom = "bar",
position = "dodge",
color = "black") +
stat_summary(fun.data = mean_cl_normal,
geom = "errorbar",
position = position_dodge(width = 0.90),
width = .2,
color = "black") +
scale_fill_manual("Task",
values = c("Judgment" = "indianred4",
"Recall" = "dimgray")) +
ylim(0, 1)
bar.plot2
tiff(filename = "mpagraph.tiff", res = 300, width = 6,
height = 6, units = 'in', compression = "lzw")
plot(bar.plot2)
dev.off()
##updated graph
bar.plot2 = ggplot(long.dat, aes(Judgment.Type, Score, fill = Task)) +
cleanup +
theme(legend.position="top") +
stat_summary(fun.y = mean,
geom = "bar",
position = "dodge",
color = "black") +
stat_summary(fun.data = mean_cl_normal,
geom = "errorbar",
position = position_dodge(width = 0.90),
width = .2,
color = "black") +
scale_fill_manual("Task",
values = c("Judgment" = "indianred4",
"Recall" = "dimgray")) +
ylim(0, 1) +
xlab("Judgment Type")
bar.plot2
##updated graph
bar.plot2 = ggplot(long.dat, aes(Judgment.Type, Score, fill = Task)) +
cleanup +
theme(legend.position=c(1,1)) +
stat_summary(fun.y = mean,
geom = "bar",
position = "dodge",
color = "black") +
stat_summary(fun.data = mean_cl_normal,
geom = "errorbar",
position = position_dodge(width = 0.90),
width = .2,
color = "black") +
scale_fill_manual("Task",
values = c("Judgment" = "indianred4",
"Recall" = "dimgray")) +
ylim(0, 1) +
xlab("Judgment Type")
bar.plot2
##updated graph
bar.plot2 = ggplot(long.dat, aes(Judgment.Type, Score, fill = Task)) +
cleanup +
theme(legend.position=c(.75,.75)) +
stat_summary(fun.y = mean,
geom = "bar",
position = "dodge",
color = "black") +
stat_summary(fun.data = mean_cl_normal,
geom = "errorbar",
position = position_dodge(width = 0.90),
width = .2,
color = "black") +
scale_fill_manual("Task",
values = c("Judgment" = "indianred4",
"Recall" = "dimgray")) +
ylim(0, 1) +
xlab("Judgment Type")
bar.plot2
##updated graph
bar.plot2 = ggplot(long.dat, aes(Judgment.Type, Score, fill = Task)) +
cleanup +
theme(legend.position=c(.9,.9)) +
stat_summary(fun.y = mean,
geom = "bar",
position = "dodge",
color = "black") +
stat_summary(fun.data = mean_cl_normal,
geom = "errorbar",
position = position_dodge(width = 0.90),
width = .2,
color = "black") +
scale_fill_manual("Task",
values = c("Judgment" = "indianred4",
"Recall" = "dimgray")) +
ylim(0, 1) +
xlab("Judgment Type")
bar.plot2
##updated graph
bar.plot2 = ggplot(long.dat, aes(Judgment.Type, Score, fill = Task)) +
cleanup +
theme(legend.position=c(.9,.9)) +
stat_summary(fun.y = mean,
geom = "bar",
position = "dodge",
color = "black") +
stat_summary(fun.data = mean_cl_normal,
geom = "errorbar",
position = position_dodge(width = 0.90),
width = .2,
color = "black") +
scale_fill_manual("Task",
values = c("Judgment" = "indianred4",
"Recall" = "dimgray")) +
coord_cartesian(ylim = c(0,1)) +
xlab("Judgment Type")
bar.plot2
tiff(filename = "mpagraph.tiff", res = 300, width = 8,
height = 6, units = 'in', compression = "lzw")
plot(bar.plot2)
dev.off()
library(rvest)
##NY Times##
#Specifying the url for desired website to be scrapped
url <- 'https://www.nytimes.com/section/politics'
#Reading the HTML code from the website - headlines
webpage <- read_html(url)
headline_data = html_nodes(webpage,'.story-link a, .story-body a')
#headline_data = html_text(headline_data) #convert to readable text
#head(headline_data) #hmmm, I think it could work
#okay, so I can get the urls this way, but idk how to get rid of the other crap
attr_data = html_attrs(headline_data)
attr_data
urlslist = unlist(attr_data)
urlslist = urlslist[grep("http", urlslist)]
urlslist = unique(urlslist)
urlslist
##start a data frame
NYtimesDF = matrix(NA, nrow = length(urlslist), ncol = 3)
colnames(NYtimesDF) = c("Source", "Url", "Text")
NYtimesDF = as.data.frame(NYtimesDF)
##for loops
for (i in 1:length(urlslist)){
##read in the URL
webpage <- read_html(urlslist[i])
##pull the specific nodes
headline_data = html_nodes(webpage,'.story-content')
##pull the text
text_data = html_text(headline_data)
##save the data
NYtimesDF$Source[i] = "NY Times"
NYtimesDF$Url[i] = urlslist[i]
NYtimesDF$Text[i] = paste(text_data, collapse = "")
} ##end for loop
#Bueno, aquí está la página politica de NPR
url2 = 'https://www.npr.org/sections/politics/'
webpage2 = read_html(url2)
headline_data2 = html_nodes(webpage2,'.title a')
#headline_data = html_text(headline_data)
#head(headline_data) #Very nice. NPR Politics does not show a huge list on its front page
#Los Localizadores Uniformes de Recursos (URLs)
attr_data2 = html_attrs(headline_data2)
attr_data2
urlslist2 = unlist(attr_data2)
urlslist2 = urlslist2[grep("http", urlslist2)]
urlslist2
##start a data frame
NPRDF = matrix(NA, nrow = length(urlslist2), ncol = 3)
colnames(NPRDF) = c("Source", "Url", "Text")
NPRDF = as.data.frame(NPRDF)
##for loops
for (i in 1:length(urlslist2)){
##read in the URL
webpage2 <- read_html(urlslist2[i])
##pull the specific nodes
headline_data2 = html_nodes(webpage2,'#storytext > p')
##pull the text
text_data2 = html_text(headline_data2)
##save the data
NPRDF$Source[i] = "NPR"
NPRDF$Url[i] = urlslist2[i]
NPRDF$Text[i] = paste(text_data2, collapse = "")
} ##end for loop
#Y ahora, el Fox News
url3 = 'http://www.foxnews.com/politics.html'
webpage3 = read_html(url3)
headline_data3 = html_nodes(webpage3, '.story- a , .article-list .title a')
#headline_data = html_text(headline_data)
head(headline_data3) #doesn't look bad, the output that is...
attr_data3 = html_attrs(headline_data3)
attr_data3
urlslist3 = unlist(attr_data3)
##find all that are href
urlslist3 = urlslist3[grep("http|.html", urlslist3)]
##fix the ones without the leading foxnews.com
urlslist3F = paste("http://www.foxnews.com", urlslist3[grep("^http", urlslist3, invert = T)], sep = "")
urlslist3N = urlslist3[grep("^http", urlslist3)]
urlslist3 = c(urlslist3N, urlslist3F)
urlslist3 = unique(urlslist3)
urlslist3
##start a data frame
FoxDF = matrix(NA, nrow = length(urlslist3), ncol = 3)
colnames(FoxDF) = c("Source", "Url", "Text")
FoxDF = as.data.frame(FoxDF)
##for loops
for (i in 1:length(urlslist3)){
##read in the URL
webpage3 <- read_html(urlslist3[i])
##pull the specific nodes
headline_data3 = html_nodes(webpage3,'p+ p , .fn-video+ p , .twitter+ p , .speakable')
##pull the text
text_data3 = html_text(headline_data3)
##save the data
FoxDF$Source[i] = "Fox News"
FoxDF$Url[i] = urlslist3[i]
FoxDF$Text[i] = paste(text_data3, collapse = "")
} ##end for loop
#Oh God, it's time to do Breitbart :(
url4 = 'http://www.breitbart.com/big-government/'
webpage4 = read_html(url4)
headline_data4 = html_nodes(webpage4, '.title a , #grid-block-0 span , #BBTrendUL a , #disqus-popularUL a , font')
#headline_data4 = html_text(headline_data4)
head(headline_data4)
attr_data4 = html_attrs(headline_data4)
attr_data4
urlslist4 = unlist(attr_data4)
##find all that are href
urlslist4 = urlslist4[grep("http|/big-government", urlslist4)]
##fix the ones without the leading bb
urlslist4F = paste("http://www.breitbart.com", urlslist4[grep("^http", urlslist4, invert = T)], sep = "")
urlslist4N = urlslist4[grep("^http", urlslist4)]
urlslist4 = c(urlslist4N, urlslist4F)
urlslist4 = unique(urlslist4)
urlslist4
##start a data frame
BreitbartDF = matrix(NA, nrow = length(urlslist4), ncol = 3)
colnames(BreitbartDF) = c("Source", "Url", "Text")
BreitbartDF = as.data.frame(BreitbartDF)
##for loops
for (i in 1:length(urlslist4)){
##read in the URL
webpage4 <- read_html(urlslist4[i])
##pull the specific nodes
headline_data4 = html_nodes(webpage4,'.entry-content p , h2')
##pull the text
text_data4 = html_text(headline_data4)
##save the data
BreitbartDF$Source[i] = "Breitbart"
BreitbartDF$Url[i] = urlslist4[i]
BreitbartDF$Text[i] = paste(text_data4, collapse = "")
} ##end for loop
##fix the above so they all look like NYtimesDF (with different names)
##set your working directory
setwd("~/OneDrive - Missouri State University/RESEARCH/2 projects/Will-Pilot")
##import the overalldata
overalldata = read.csv("overalldata.csv")
##combine with new data (add the other DF names in here...)
newdata = rbind(overalldata, NYtimesDF, NPRDF, FoxDF, BreitbartDF)
##make the newdata unique in case of overlap across days
newdata = unique(newdata)
##write it back out
write.csv(newdata, "overalldata.csv", row.names = F)
##number of articles
table(newdata$Source)
##number of words
library(ngram)
newdata$Text = as.character(newdata$Text)
for (i in 1:nrow(newdata)) {
#newdata$writing[i] = preprocess(newdata$Text[i], case="lower", remove.punct=TRUE)
newdata$wordcount[i] = string.summary(newdata$Text[i])$words
}
tapply(newdata$wordcount, newdata$Source, mean)
tapply(newdata$wordcount, newdata$Source, sum)
shiny::runApp('~/OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/MOTE')
library(MOTE)
?d.to.r
d.to.r(d = .5, n1 = 50, n2 = 50, a = .05)
runApp('~/OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/MOTE')
d.to.r
d.to.r(d = -.5, n1 = 50, n2 = 50, a = .05)
-.5^2
(-.5^2)
(-.5)^2
r = -.5
rsq <- (r) ^ 2
rsq
d.to.r <- function (d, n1, n2, a = .05) {
# This function Displays transformation from r to r2 to calculate
# the non-central confidence interval for r2.
#
# Args:
#   d   : effect size statistic
#   n1  : sample size group one
#   n2  : sample size group two
#   a   : significance level
#
# Returns:
#   List of r, r2, and sample size statistics
library(MBESS)
correct = (n1 + n2)^2 / (n1*n2)
n = n1 + n2
r <- d / sqrt(d^2 + correct)
rsq <- (r) ^ 2
se <- sqrt(4 * rsq * ((1 - rsq) ^ 2) * ((n - 3) ^ 2) / ((n ^ 2 - 1) * (3 + n)))
t <- r / sqrt((1 - rsq) / (n - 2))
Fvalue <- t ^ 2
dfm <- 1
dfe <- n - 2
ncpboth <- conf.limits.ncf(Fvalue, df.1 = dfm, df.2 = dfe, conf.level = (1 - a))
rsqlow <- ncpboth$Lower.Limit / (ncpboth$Lower.Limit + dfm + dfe + 1)
rsqhigh <- ncpboth$Upper.Limit / (ncpboth$Upper.Limit + dfm + dfe + 1)
ciforr <- ci.R(R = abs(r), df.1 = dfm, df.2 = dfe, conf.level = (1 - a))
p <- pf(Fvalue, dfm, dfe, lower.tail = F)
output = list("r" = r, #r stats
"rlow" = ciforr$Lower.Conf.Limit.R,
"rhigh" = ciforr$Upper.Conf.Limit.R,
"R2" = rsq, #R squared stats
"R2low" = rsqlow,
"R2high" = rsqhigh,
"se" = se,
"n" = n, #sample stats
"dfm" = 1, #sig stats
"dfe" = (n - 2),
"t" = t,
"F" = Fvalue,
"p" = p)
return(output)
}
d.to.r(d = -.5, n1 = 50, n2 = 50, a = .05)
rlow = .04
0 - rlow
rlow = -.04
0 - row
0 - rlow
d.to.r <- function (d, n1, n2, a = .05) {
# This function Displays transformation from r to r2 to calculate
# the non-central confidence interval for r2.
#
# Args:
#   d   : effect size statistic
#   n1  : sample size group one
#   n2  : sample size group two
#   a   : significance level
#
# Returns:
#   List of r, r2, and sample size statistics
library(MBESS)
correct = (n1 + n2)^2 / (n1*n2)
n = n1 + n2
r <- d / sqrt(d^2 + correct)
rsq <- (r) ^ 2
se <- sqrt(4 * rsq * ((1 - rsq) ^ 2) * ((n - 3) ^ 2) / ((n ^ 2 - 1) * (3 + n)))
t <- r / sqrt((1 - rsq) / (n - 2))
Fvalue <- t ^ 2
dfm <- 1
dfe <- n - 2
ncpboth <- conf.limits.ncf(Fvalue, df.1 = dfm, df.2 = dfe, conf.level = (1 - a))
rsqlow <- ncpboth$Lower.Limit / (ncpboth$Lower.Limit + dfm + dfe + 1)
rsqhigh <- ncpboth$Upper.Limit / (ncpboth$Upper.Limit + dfm + dfe + 1)
ciforr <- ci.R(R = abs(r), df.1 = dfm, df.2 = dfe, conf.level = (1 - a))
p <- pf(Fvalue, dfm, dfe, lower.tail = F)
#deal with negative r / d values
if (r < 0) {
rlow = 0 - ciforr$Lower.Conf.Limit.R
rhigh = 0 - ciforr$Upper.Conf.Limit.R
} else {
rlow = ciforr$Lower.Conf.Limit.R
rhigh = ciforr$Upper.Conf.Limit.R
}
output = list("r" = r, #r stats
"rlow" = rlow,
"rhigh" = rhigh,
"R2" = rsq, #R squared stats
"R2low" = rsqlow,
"R2high" = rsqhigh,
"se" = se,
"n" = n, #sample stats
"dfm" = 1, #sig stats
"dfe" = (n - 2),
"t" = t,
"F" = Fvalue,
"p" = p)
return(output)
}
d.to.r(-.5,50,50,.06)
##set your working directory to package file
setwd("~/OneDrive - Missouri State University/RESEARCH/2 projects/MOTE")
##run this thing
library(roxygen2)
roxygen2::roxygenise()
##then run this to update
devtools::install_github("doomlab/MOTE")
runApp('~/OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/MOTE')
