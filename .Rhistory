runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
shiny::runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lab/single_words')
library(lavaan)
data(HolzingerSwineford1939)
visual.model = 'visual =~ x1 + x2 + x3'
visual.fit = cfa(model = visual.model, data = HolzingerSwineford1939)
summary(visual.fit)
visual.model = 'visual =~ x1 + x2 + x3 + x4'
visual.fit = cfa(model = visual.model, data = HolzingerSwineford1939)
summary(visual.fit)
visual.model = 'visual =~ x4 + x5 + x6'
visual.fit = cfa(model = visual.model, data = HolzingerSwineford1939)
summary(visual.fit)
summary(visual.fit, standardized = T)
summary(visual.fit, standardized = T)
summary(visual.fit, standardized = T, fit.measures = TRUE)
visual.model = 'visual =~ x1 + x2 + x3 + x7+x8+x9'
visual.fit = cfa(model = visual.model, data = HolzingerSwineford1939)
summary(visual.fit, standardized = T, fit.measures = TRUE)
summary(visual.fit, standardized = T, fit.measures = TRUE)
library(lavaan)
help(PoliticalDemocracy)
data(lavaan)
library(lavaan)
text.model <- 'textspeed =~ x4 + x5 + x6 + x7 + x8 + x9'
cfa(text.model, data = HolzingerSwineford1939)
library(lavaan)
data(HolzingerSwineford1939)
text.model <- 'textspeed =~ x4 + x5 + x6 + x7 + x8 + x9'
text.fit <- cfa(model = text.model, data = HolzingerSwineford1939)
summary(text.fit)
lab_table <- read.csv("~/OneDrive - Missouri State University/RESEARCH/2 projects/Table of Doom/website update/lab_table.csv", comment.char="#")
View(lab_table)
ncol(lab_table)
names(lab_table)
library(lavaan)
data(PoliticalDemocracy)
politics.model <- 'poldemo60 =~ y1 + y2 + y3 + y4'
politics.fit <- cfa(model = politics.model, data = PoliticalDemocracy)
summary(politics.fit)
library(lavaan)
data(PoliticalDemocracy)
politics.model <- 'poldemo60 =~ y1 + y2 + y3 + y4'
politics.fit <- cfa(model = politics.model, data = PoliticalDemocracy)
summary(politics.fit)
library(lavaan)
data(HolzingerSwineford1939)
text.model <- 'textspeed =~ x4 + x5 + x6 + x7 + x8 + x9'
text.fit <- cfa(model = text.model, data = HolzingerSwineford1939)
summary(text.fit, standardized = TRUE)
library(lavaan)
data(HolzingerSwineford1939)
text.model <- 'textspeed =~ x4 + x5 + x6 + x7 + x8 + x9'
text.fit <- cfa(model = text.model, data = HolzingerSwineford1939)
summary(text.fit, fit.measures = TRUE)
library(lavaan)
data(PoliticalDemocracy)
politics.model <- 'poldemo60 =~ y1 + y2 + y3 + y4'
politics.fit <- cfa(model = politics.model, data = PoliticalDemocracy)
summary(politics.fit, standardized = TRUE, fit.measures = TRUE)
install.packages(c("ade4", "afex", "backports", "BatchExperiments", "BatchJobs", "BDgraph", "BH", "bookdown", "broom", "callr", "car", "checkmate", "coin", "cowplot", "curl", "data.table", "dbplyr", "DescTools", "devtools", "DT", "dunn.test", "ellipse", "epitools", "estimability", "FactoMineR", "foreach", "FSA", "git2r", "glue", "haven", "hexbin", "Hmisc", "hms", "htmlTable", "htmlwidgets", "irlba", "iterators", "jmv", "jmvcore", "kableExtra", "knitr", "lhs", "lme4", "lmerTest", "lsmeans", "lubridate", "MASS", "matrixStats", "MBESS", "memisc", "meta", "mgcv", "mice", "mirt", "mixOmics", "mokken", "msm", "multcomp", "MuMIn", "mvtnorm", "OpenMx", "openssl", "pbapply", "plotrix", "PMCMR", "powerMediation", "quantmod", "quantreg", "rcompanion", "RcppArmadillo", "RcppEigen", "RCurl", "registry", "reprex", "rgl", "rockchalk", "rpart", "rpf", "rprojroot", "RVAideMemoire", "slam", "spatstat", "spatstat.data", "spatstat.utils", "StanHeaders", "stringi", "survey", "tidyr", "tidyselect", "timeDate", "tm", "TOSTER", "tseries", "TTR", "vcd", "vegan", "xaringan", "xml2", "xts", "yaml", "zoo"))
install.packages(c("ade4", "afex", "backports", "BatchExperiments", "BatchJobs", "BDgraph", "BH", "bookdown", "broom", "callr", "car", "checkmate", "coin", "cowplot", "curl", "data.table", "dbplyr", "DescTools", "devtools", "DT", "dunn.test", "ellipse", "epitools", "estimability", "FactoMineR", "foreach", "FSA", "git2r", "glue", "haven", "hexbin", "Hmisc", "hms", "htmlTable", "htmlwidgets", "irlba", "iterators", "jmv", "jmvcore", "kableExtra", "knitr", "lhs", "lme4", "lmerTest", "lsmeans", "lubridate", "MASS", "matrixStats", "MBESS", "memisc", "meta", "mgcv", "mice", "mirt", "mixOmics", "mokken", "msm", "multcomp", "MuMIn", "mvtnorm", "OpenMx", "openssl", "pbapply", "plotrix", "PMCMR", "powerMediation", "quantmod", "quantreg", "rcompanion", "RcppArmadillo", "RcppEigen", "RCurl", "registry", "reprex", "rgl", "rockchalk", "rpart", "rpf", "rprojroot", "RVAideMemoire", "slam", "spatstat", "spatstat.data", "spatstat.utils", "StanHeaders", "stringi", "survey", "tidyr", "tidyselect", "timeDate", "tm", "TOSTER", "tseries", "TTR", "vcd", "vegan", "xaringan", "xml2", "xts", "yaml", "zoo"))
install.packages(c("ade4", "afex", "backports", "BatchExperiments", "BatchJobs", "BDgraph", "BH", "bookdown", "broom", "callr", "car", "checkmate", "coin", "cowplot", "curl", "data.table", "dbplyr", "DescTools", "devtools", "DT", "dunn.test", "ellipse", "epitools", "estimability", "FactoMineR", "foreach", "FSA", "git2r", "glue", "haven", "hexbin", "Hmisc", "hms", "htmlTable", "htmlwidgets", "irlba", "iterators", "jmv", "jmvcore", "kableExtra", "knitr", "lhs", "lme4", "lmerTest", "lsmeans", "lubridate", "MASS", "matrixStats", "MBESS", "memisc", "meta", "mgcv", "mice", "mirt", "mixOmics", "mokken", "msm", "multcomp", "MuMIn", "mvtnorm", "OpenMx", "openssl", "pbapply", "plotrix", "PMCMR", "powerMediation", "quantmod", "quantreg", "rcompanion", "RcppArmadillo", "RcppEigen", "RCurl", "registry", "reprex", "rgl", "rockchalk", "rpart", "rpf", "rprojroot", "RVAideMemoire", "slam", "spatstat", "spatstat.data", "spatstat.utils", "StanHeaders", "stringi", "survey", "tidyr", "tidyselect", "timeDate", "tm", "TOSTER", "tseries", "TTR", "vcd", "vegan", "xaringan", "xml2", "xts", "yaml", "zoo"))
install.packages(c("ade4", "afex", "backports", "BatchExperiments", "BatchJobs", "BDgraph", "BH", "bookdown", "broom", "callr", "car", "checkmate", "coin", "cowplot", "curl", "data.table", "dbplyr", "DescTools", "devtools", "DT", "dunn.test", "ellipse", "epitools", "estimability", "FactoMineR", "foreach", "FSA", "git2r", "glue", "haven", "hexbin", "Hmisc", "hms", "htmlTable", "htmlwidgets", "irlba", "iterators", "jmv", "jmvcore", "kableExtra", "knitr", "lhs", "lme4", "lmerTest", "lsmeans", "lubridate", "MASS", "matrixStats", "MBESS", "memisc", "meta", "mgcv", "mice", "mirt", "mixOmics", "mokken", "msm", "multcomp", "MuMIn", "mvtnorm", "OpenMx", "openssl", "pbapply", "plotrix", "PMCMR", "powerMediation", "quantmod", "quantreg", "rcompanion", "RcppArmadillo", "RcppEigen", "RCurl", "registry", "reprex", "rgl", "rockchalk", "rpart", "rpf", "rprojroot", "RVAideMemoire", "slam", "spatstat", "spatstat.data", "spatstat.utils", "StanHeaders", "stringi", "survey", "tidyr", "tidyselect", "timeDate", "tm", "TOSTER", "tseries", "TTR", "vcd", "vegan", "xaringan", "xml2", "xts", "yaml", "zoo"))
install.packages(c("ade4", "afex"))
install.packages(c("backports", "BatchExperiments"))
install.packages(c("BDgraph", "BH", "bookdown", "broom"))
install.packages(c("callr", "car", "coin", "cowplot", "curl", "data.table", "dbplyr", "DescTools", "devtools", "DT", "dunn.test", "ellipse", "epitools", "estimability", "FactoMineR", "foreach", "FSA", "git2r", "glue", "haven", "hexbin", "Hmisc", "hms", "htmlTable", "htmlwidgets", "irlba", "iterators", "jmv", "jmvcore", "kableExtra", "knitr", "lhs", "lme4", "lmerTest", "lsmeans", "lubridate", "MASS", "matrixStats", "MBESS", "memisc", "meta", "mgcv", "mice", "mirt", "mixOmics", "mokken", "msm", "multcomp", "MuMIn", "mvtnorm", "OpenMx", "openssl", "pbapply", "plotrix", "PMCMR", "powerMediation", "quantmod", "quantreg", "rcompanion", "RcppArmadillo", "RcppEigen", "RCurl", "registry", "reprex", "rgl", "rockchalk", "rpart", "rpf", "rprojroot", "RVAideMemoire", "slam", "spatstat", "spatstat.data", "spatstat.utils", "StanHeaders", "stringi", "survey", "tidyr", "tidyselect", "timeDate", "tm", "TOSTER", "tseries", "TTR", "vcd", "vegan", "xaringan", "xml2", "xts", "yaml", "zoo"))
shiny::runApp('OneDrive - Missouri State University/RESEARCH/2 projects/shiny-server/lq-screen')
library(lavaan)
library(lavaan)
data(HolzingerSwineford1939)
text.model <- 'textspeed =~ x4 + x5 + x6 + x7 + x8 + x9'
text.fit <- cfa(model = text.model, data = HolzingerSwineford1939)
summary(text.fit, fit.measures = TRUE)
library(lavaan)
data(HolzingerSwineford1939)
text.model <- 'textspeed =~ x4 + x5 + x6
speed =~ x7 + x8 + x9'
text.fit <- cfa(model = text.model, data = HolzingerSwineford1939)
summary(text.fit, fit.measures = TRUE)
text.model <- 'textspeed =~ x4 + 1*x5 + x6
speed =~ x7 + x8 + x9'
text.fit <- cfa(model = text.model, data = HolzingerSwineford1939)
summary(text.fit, fit.measures = TRUE)
library(lavaan)
visual.model <- 'visual =~ x1 + x2 + x3 + x7 + x8 + x9'
visual.model = cfa(model = visual.model, data = HolzingerSwineford1939)
visual.model <- 'visual =~ x1 + x2 + x3 + x7 + x8 + x9'
visual.fit = cfa(model = visual.model, data = HolzingerSwineford1939)
summary(visual.fit, standardized = T, fit.measures = T)
> visual.model <- 'visual =~ x1 + x2 + x3'
> visual.fit <- cfa(model = visual.model,
+                  data = HolzingerSwineford1939)
> summary(visual.fit, standardized = TRUE, fit.measures = TRUE)
visual.model <- 'visual =~ x1 + x2 + x3'
visual.fit <- cfa(model = visual.model,
data = HolzingerSwineford1939)
summary(visual.fit, standardized = TRUE, fit.measures = TRUE)
visual.model <- 'visual =~ x1 + a*x2 + a*x3'
visual.fit <- cfa(model = visual.model,
data = HolzingerSwineford1939)
summary(visual.fit, standardized = TRUE, fit.measures = TRUE)
visual.fit <- cfa(model = visual.model,
data = HolzingerSwineford1939)
summary(visual.fit, standardized = TRUE, fit.measures = TRUE)
twofactor.model <- 'visual =~ x1 + x2 + x3
speed =~ x7 + x8 + x9'
twofactor.fit <- cfa(model = twofactor.model,
data = HolzingerSwineford1939)
summary(twofactor.fit, standardized = TRUE, fit.measures = TRUE)
twofactor.fit <- cfa(model = twofactor.model,
data = HolzingerSwineford1939)
summary(twofactor.fit, standardized = TRUE, fit.measures = TRUE)
knitr::opts_chunk$set(cache = TRUE)
library(papaja)
library(car)
library(memisc)
library(moments)
library(mice)
library(monomvn)
library(psych)
library(TOSTER)
library(MOTE)
library(BayesFactor)
####import the files####
PIL = read.csv("PIL.csv")
##reverse coding the PIL
PIL[ , c("PIL2", "PIL5", "PIL7", "PIL10", "PIL14", "PIL15", "PIL17", "PIL18", "PIL19")] = 8 - PIL[ , c("PIL2", "PIL5","PIL7", "PIL10", "PIL14", "PIL15", "PIL17", "PIL18", "PIL19")]
##subsetting the PIL dataset
nr = subset(PIL, Source == "not random")
r = subset(PIL, Source == "random")
######Random data screening#####
randomcomputer = r[, c(3:22)]
summary(randomcomputer)
apply(randomcomputer, 2, table)
##missing data
##participants (row)
percentmiss = function(x) { sum(is.na(x))/length(x)*100}
missingrandomcomputer = apply(randomcomputer, 1, percentmiss)
table(missingrandomcomputer)
replacepeoplerandom = subset(randomcomputer, missingrandomcomputer <= 5)
summary(replacepeoplerandom)
##variables (column)
apply(replacepeoplerandom, 2, percentmiss)
replacecolumnrandom = replacepeoplerandom[ , -15]
dontcolumnrrandom = replacepeoplerandom[ , 15]
##replace data
tempnomissrandom = mice(replacecolumnrandom)
?mice
##replace data
tempnomissrandom = mice(replacecolumnrandom, seed = 165)
knitr::opts_chunk$set(cache = TRUE)
library(papaja)
library(car)
library(memisc)
library(moments)
library(mice)
library(monomvn)
library(psych)
library(TOSTER)
library(MOTE)
library(BayesFactor)
####import the files####
PIL = read.csv("PIL.csv")
##reverse coding the PIL
PIL[ , c("PIL2", "PIL5", "PIL7", "PIL10", "PIL14", "PIL15", "PIL17", "PIL18", "PIL19")] = 8 - PIL[ , c("PIL2", "PIL5","PIL7", "PIL10", "PIL14", "PIL15", "PIL17", "PIL18", "PIL19")]
##subsetting the PIL dataset
nr = subset(PIL, Source == "not random")
r = subset(PIL, Source == "random")
######Random data screening#####
randomcomputer = r[, c(3:22)]
summary(randomcomputer)
apply(randomcomputer, 2, table)
##missing data
##participants (row)
percentmiss = function(x) { sum(is.na(x))/length(x)*100}
missingrandomcomputer = apply(randomcomputer, 1, percentmiss)
table(missingrandomcomputer)
replacepeoplerandom = subset(randomcomputer, missingrandomcomputer <= 5)
summary(replacepeoplerandom)
##variables (column)
apply(replacepeoplerandom, 2, percentmiss)
replacecolumnrandom = replacepeoplerandom[ , -15]
dontcolumnrrandom = replacepeoplerandom[ , 15]
##replace data
tempnomissrandom = mice(replacecolumnrandom, seed = 165)
replacedrandom_temp = complete(tempnomissrandom, 1)
replacedrandom = cbind(replacedrandom_temp[ , 1:14],
PIL15 = dontcolumnrrandom,
replacedrandom_temp[ , 15:19])
summary(replacedrandom)
##outliers
mahalrandom = mahalanobis(replacedrandom[ , ],
colMeans(replacedrandom[ , ], na.rm = T),
cov(replacedrandom[ , ], use = "pairwise.complete.obs"))
cutoff = qchisq(1-.001, ncol(replacedrandom[ , ]))
ncol(replacedrandom[ , ])
cutoff
summary(mahalrandom < cutoff)
nooutrandom = subset(replacedrandom, mahalrandom < cutoff)
##assumptions
##additivity
correlrandom = cor(nooutrandom[ , ])
symnum(correlrandom)
finalrandomP = nooutrandom
##set up for assumptions
randomrandom = rchisq(nrow(finalrandomP), 7)
fakerandom = lm(randomrandom ~ ., data = finalrandomP)
standardizedrandom = rstudent(fakerandom)
fittedrandom = scale(fakerandom$fitted.values)
##normality
skewness(finalrandomP[ , ], na.rm = T)
kurtosis(finalrandomP[ , ], na.rm = T)
#hist(standardizedrandom)
##linearity
#qqnorm(standardizedrandom)
#abline(0,1)
##homog + s
#plot(fittedrandom, standardizedrandom)
#abline(0,0)
#abline(v = 0)
######NOTRandom data screening#####
notrandomcomputer = nr[, c(3:22)]
summary(notrandomcomputer)
apply(notrandomcomputer, 2, table)
##missing data
##participants (row)
missingnotrandomcomputer = apply(notrandomcomputer, 1, percentmiss)
table(missingnotrandomcomputer)
replacepeoplenotrandom = subset(notrandomcomputer, missingnotrandomcomputer <= 5)
summary(replacepeoplenotrandom)
##variables (columns)
apply(replacepeoplenotrandom, 2, percentmiss)
##replace the data
tempnomissnotrandom = mice(replacepeoplenotrandom, seed = 583)
replacednotrandom = complete(tempnomissnotrandom, 1)
summary(replacednotrandom)
##outliers
mahalnotrandom = mahalanobis(replacednotrandom[ , ],
colMeans(replacednotrandom[ , ], na.rm = T),
cov(replacednotrandom[ , ], use = "pairwise.complete.obs"))
cutoff = qchisq(1-.001, ncol(replacednotrandom[ , ]))
ncol(replacednotrandom[ , ])
cutoff
summary(mahalnotrandom < cutoff)
nooutnotrandom = subset(replacednotrandom, mahalnotrandom < cutoff)
##assumptions
##additivity
correlnotrandom = cor(nooutnotrandom[ , ])
symnum(correlnotrandom)
finalnotrandomP = nooutnotrandom
##set up for assumptions
randomnotrandom = rchisq(nrow(finalnotrandomP), 7)
fakenotrandom = lm(randomnotrandom ~ ., data = finalnotrandomP)
standardizednotrandom = rstudent(fakenotrandom)
fittednotrandom = scale(fakenotrandom$fitted.values)
##normality
skewness(finalnotrandomP[ , ], na.rm = T)
kurtosis(finalnotrandomP[ , ], na.rm = T)
#hist(standardizednotrandom)
##linearity
#qqnorm(standardizednotrandom)
#abline(0,1)
##homog + s
#plot(fittednotrandom, standardizednotrandom)
#abline(0,0)
#abline(v = 0)
##import the data files
LPQallcompiled = read.csv("LPQ.csv")
###changing to 0 and 1 - columns are currently 1 true, 2 false, so subtract 2
names(LPQallcompiled)
summary(LPQallcompiled)
##fix the 12 typo
LPQallcompiled$lpq16_1[ LPQallcompiled$lpq16_1 == 12] = NA
allcolumns = c("lpq1_1", "lpq2_1", "lpq3_1", "lpq4_1", "lpq5_1", "lpq6_1", "lpq7_1", "lpq8_1", "lpq9_1", "lpq10_1", "lpq11_1", "lpq12_1", "lpq13_1", "lpq14_1", "lpq15_1", "lpq16_1", "lpq17_1", "lpq18_1", "lpq19_1", "lpq20_1")
LPQallcompiled[ , allcolumns] = 2 - LPQallcompiled[ , allcolumns]
###reverse code items 1, 2, 5, 8, 9, 11, 12, 14, 15, 16, 19
reverse = c("lpq1_1", "lpq2_1", "lpq5_1", "lpq8_1", "lpq9_1", "lpq11_1", "lpq12_1", "lpq14_1", "lpq15_1", "lpq16_1", "lpq19_1")
LPQallcompiled[ , reverse] = 1 - LPQallcompiled[ , reverse]
summary(LPQallcompiled)
##make sure this dataaset doesn't have any decimals or weird numbers
apply(LPQallcompiled[ , allcolumns], 2, table)
##the csv files are a mix of computer and paper, so need to subset
LPQallcompiledrandom = subset(LPQallcompiled, Source == 'random')
LPQallcompilednotrandom = subset(LPQallcompiled, Source == 'not random')
####Random data screening####
LPQallcompiledrandom = LPQallcompiledrandom[ , 3:22]
summary(LPQallcompiledrandom)
##missing lpqallcompiled random
missingallcompiledrandom = apply(LPQallcompiledrandom, 1, percentmiss)
table(missingallcompiledrandom)
replacepeopleallcompiledrandom = subset(LPQallcompiledrandom, missingallcompiledrandom <=5)
##check for columns
apply(replacepeopleallcompiledrandom, 2, percentmiss)
##replace data
tempnomiss = mice(replacepeopleallcompiledrandom, seed = 784)
replacedallcompiledrandom = complete(tempnomiss, 1)
summary(replacedallcompiledrandom)
##outliers
mahal = mahalanobis(replacedallcompiledrandom,
colMeans(replacedallcompiledrandom, na.rm = T),
cov(replacedallcompiledrandom, use = "pairwise.complete.obs"))
cutoff = qchisq(1-.001, ncol(replacedallcompiledrandom))
cutoff
ncol(replacedallcompiledrandom)
summary(mahal < cutoff)
nooutreplacedallcompiledrandom = subset(replacedallcompiledrandom, mahal < cutoff)
##assumptions
##additivity
correlreplacedallcompiledrandom = cor(nooutreplacedallcompiledrandom)
symnum(correlreplacedallcompiledrandom) ##good
##set up for assumptions
random = rchisq(nrow(nooutreplacedallcompiledrandom), 7)
fake = lm(random ~ ., data = nooutreplacedallcompiledrandom)
standardized = rstudent(fake)
fitted = scale(fake$fitted.values)
##normality
#hist(standardized) ##positive skew
##linearity
#qqnorm(standardized)
#abline(0, 1) ##looks good
##homog and s
#plot(fitted, standardized)
#abline(0, 0)
#abline(v = 0)##yes!
finalrandomL = nooutreplacedallcompiledrandom
####Not random data screening####
LPQallcompilednotrandom = LPQallcompilednotrandom[ , 3:22]
summary(LPQallcompilednotrandom)
##missing lpqallcompilednotrandom
missingallcompilednotrandom = apply(LPQallcompilednotrandom, 1, percentmiss)
table(missingallcompilednotrandom)
replacepeopleallcompilednotrandom = subset(LPQallcompilednotrandom, missingallcompilednotrandom <= 5)
##columns
apply(replacepeopleallcompilednotrandom,2,percentmiss)
##replace data
tempnomiss = mice(replacepeopleallcompilednotrandom, seed = 489)
replacedallcompilednotrandom = complete(tempnomiss, 1)
summary(replacedallcompilednotrandom)
##outliers
mahal = mahalanobis(replacedallcompilednotrandom,
colMeans(replacedallcompilednotrandom, na.rm = T),
cov(replacedallcompilednotrandom, use = "pairwise.complete.obs"))
cutoff = qchisq(1-.001, ncol(replacedallcompilednotrandom))
cutoff
ncol(replacedallcompilednotrandom)
summary(mahal < cutoff)
nooutreplacedallcompilednotrandom = subset(replacedallcompilednotrandom, mahal < cutoff)
##assumptions
##additivity
correlreplacedallcompilednotrandom = cor(nooutreplacedallcompilednotrandom)
symnum(correlreplacedallcompilednotrandom) ##yay
##set up for assumptions
random = rchisq(nrow(nooutreplacedallcompilednotrandom), 7)
fake = lm(random ~ ., data = nooutreplacedallcompilednotrandom)
standardized = rstudent(fake)
fitted = scale(fake$fitted.values)
##normality
#hist(standardized) ##positive skew still
##linearity
#qqnorm(standardized)
#abline(0, 1) ##its okay
##homog and s
#plot(fitted, standardized)
#abline(0, 0)
#abline(v = 0)##meets both
finalnotrandomL = nooutreplacedallcompilednotrandom
demo = read.csv("demographics.csv")
gender = tapply(demo$Gender, demo$Source, table)
ethnic = tapply(demo$Ethnicity, demo$Source, table)
Mage = tapply(demo$Age, demo$Source, mean, na.rm = T)
SDage = tapply(demo$Age, demo$Source, sd, na.rm = T)
##make a blank table
demotable = matrix(NA, nrow = 4, ncol = 7)
##create column names
colnames(demotable) = c("Group", "Female", "White", "Age (SD)", "Original N", "Missing N", "Outlier N")
##stick in the information you need
demotable[ , 1] = c("PIL Random", "PIL Not Random", "LPQ Random", "LPQ Not Random")
demotable[ , 2] = c(apa(gender$random["female"]/sum(gender$random)*100,1),
apa(gender$`not random`["female"] / sum(gender$`not random`)*100, 1),
"-", "-")
demotable[ , 3] = c(apa(ethnic$random["White"] / sum(ethnic$random)*100, 1),
apa(ethnic$`not random`["White"] / sum(ethnic$`not random`)*100, 1),
"-", "-")
demotable[ , 4] = c(paste( apa(Mage["random"],2), " (", apa(SDage["random"],2), ")", sep = ""),
paste( apa(Mage["not random"],2), " (", apa(SDage["not random"], 2), ")", sep = ""),
"-", "-")
demotable[ , 5] = c(nrow(randomcomputer), nrow(notrandomcomputer),
nrow(LPQallcompiledrandom), nrow(LPQallcompilednotrandom))
demotable[ , 6] = as.numeric(demotable[ , 5]) -
c(nrow(replacedrandom) , nrow(replacednotrandom),
nrow(replacedallcompiledrandom), nrow(replacedallcompilednotrandom))
demotable[ , 7] = as.numeric(demotable[ , 5]) - as.numeric(demotable[ , 6]) -
c(nrow(finalrandomP), nrow(finalnotrandomP),
nrow(finalrandomL), nrow(finalnotrandomL))
##note for PIL random 256 of those are the bad data with one question, so added that to the missing column.
demotable[1, 6] = as.numeric(demotable[1, 6]) + 256
demotable[1, 7] = as.numeric(demotable[1, 7]) - 256
##print it (apa)
apa_table(
demotable
, align = c("l", rep("c", 6))
, caption = "Demographic and Data Screening Information",
note = "Participants took both the PIL and LPQ scale, therefore, random and not random demographics are the same. Not every participant was given the LPQ, resulting in missing data for those subjects. Several PIL participants were removed because they were missing an item on their scale."
)
###make covariance tables
not_corP = cov(finalnotrandomP, use="pairwise")
rand_corP = cov(finalrandomP, use="pairwise")
#mean tables
notrandommP = unlist(sapply(finalnotrandomP, function(cl) list(means=mean(cl,na.rm=TRUE))))
randommP = unlist(sapply(finalrandomP, function(cl) list(means=mean(cl,na.rm=TRUE))))
##rsme mean, cov, mean, cov
RMSEP = rmse.muS(notrandommP, not_corP, randommP, rand_corP)
##formula is cov - cov / sqrt ( var one - var other )
#do var 1 - var 1.1, then var 2 - var 2.2, then take the variance of that vector
not_vP = apply(finalnotrandomP, 2, var)
rand_vP = apply(finalrandomP, 2, var)
##figure out how to report these since you can't just divide by 2 - use csvs to figure out which and how many are over.
##sum up the standardized residuals that are over 1.96
stdP = (not_corP - rand_corP) / sqrt(var(not_vP - rand_vP))
overP = sum(as.numeric(abs(stdP)) > 1.96)
overP
##print out the standardized residuals to put online
write.csv(stdP, "stdres_not_randomP.csv")
?cortest.mat
x <- matrix(rnorm(1000),ncol=10)
cortest.normal(x)  #just test if this matrix is an identity
x <- sim.congeneric(loads =c(.9,.8,.7,.6,.5),N=1000,short=FALSE)
y <- sim.congeneric(loads =c(.9,.8,.7,.6,.5),N=1000,short=FALSE)
cortest.normal(x$r,y$r,n1=1000,n2=1000) #The Steiger test
cortest.jennrich(x$r,y$r,n1=100,n2=1000) # The Jennrich test
cortest.mat(x$r,y$r,n1=1000,n2=1000)   #twice the degrees of freedom as the Jennrich
twofactor.model <- 'visual =~ x1 + x2 + x3
speed =~ x7 + x8 + x9'
twofactor.fit <- cfa(model = twofactor.model,
data = HolzingerSwineford1939)
summary(twofactor.fit, standardized = TRUE, fit.measures = TRUE)
summary(twofactor.fit, standardized = TRUE, fit.measures = TRUE, rsquare = TRUE)
twofactor.model <- 'visual =~ x1 + x2 + x3
speed =~ x7 + x8 + x9
visual~~0*speed'
twofactor.fit <- cfa(model = twofactor.model,
data = HolzingerSwineford1939)
summary(twofactor.fit, standardized = TRUE, fit.measures = TRUE, rsquare = TRUE)
twofactor.model <- 'visual =~ x1 + x2 + x3
speed =~ x7 + x8 + x9
visual~speed'
twofactor.fit <- cfa(model = twofactor.model,
data = HolzingerSwineford1939)
summary(twofactor.fit, standardized = TRUE, fit.measures = TRUE, rsquare = TRUE)
?HolzingerSwineford1939
twofactor.model <- 'visual =~ x1 + x2 + x3
speed =~ x7 + x8 + x9
visual~~0*speed'
twofactor.fit <- cfa(model = twofactor.model,
data = HolzingerSwineford1939)
summary(twofactor.fit, standardized = TRUE, fit.measures = TRUE, rsquare = TRUE)
twofactor.model <- 'visual =~ x1 + x2 + x3
speed =~ x7 + x8 + x9
speed~visual'
twofactor.fit <- cfa(model = twofactor.model,
data = HolzingerSwineford1939)
summary(twofactor.fit, standardized = TRUE, fit.measures = TRUE, rsquare = TRUE)
x2 = 12
n = 100
dfsmall = 3
sqrt(x2/n * dfsmall)
sqrt(x2/(n * dfsmall))
x2/n
x2/n * dfsmall
(n * dfsmall)
x2/(n * dfsmall)
##set your working directory to package file
setwd("~/OneDrive - Missouri State University/RESEARCH/2 projects/MOTE")
##run this thing
library(roxygen2)
roxygen2::roxygenise()
##then run this to update
devtools::install_github("doomlab/MOTE")
library(MOTE)
?v.chi.sq
v.chi.sq
